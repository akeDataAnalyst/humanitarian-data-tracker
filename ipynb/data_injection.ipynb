{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "284a50be-95a6-426a-93c0-d17bc1ebc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d2a1d-82cd-498e-9ec9-d8e39e0e2ee4",
   "metadata": {},
   "source": [
    "#### 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "137143f5-7d09-49ea-b041-152e2f6537c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MYSQL_USER = os.getenv('MYSQL_USER')\n",
    "MYSQL_PASSWORD = os.getenv('MYSQL_PASSWORD')\n",
    "MYSQL_HOST = os.getenv('MYSQL_HOST')\n",
    "MYSQL_DATABASE = os.getenv('MYSQL_DATABASE')\n",
    "CSV_FILE = 'synthetic_gts_survey_data.csv'\n",
    "\n",
    "# SQLAlchemy Connection String\n",
    "DATABASE_URL = (\n",
    "    f\"mysql+mysqlconnector://{MYSQL_USER}:{MYSQL_PASSWORD}@\"\n",
    "    f\"{MYSQL_HOST}/{MYSQL_DATABASE}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80416c-07c5-49b8-8f12-584f6490842b",
   "metadata": {},
   "source": [
    "#### 2. Transformation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acde090c-f31e-428f-b434-9f0e7a132f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df_raw):\n",
    "    \"\"\"\n",
    "    Applies data quality control, standardization, and imputation.\n",
    "    Returns the clean DataFrame (df_clean).\n",
    "    \"\"\"\n",
    "    df_clean = df_raw.copy()\n",
    "    \n",
    "    # Configuration\n",
    "    gender_map = {'Female': 'Female', 'Male': 'Male', 'male': 'Male', 'fEmAlE': 'Female'}\n",
    "    aid_provider_map = {\n",
    "        'UNHCR ': 'UNHCR', 'Intl Rescue Commitee': 'IRC', \n",
    "        'WFP': 'WFP', 'UNICEF': 'UNICEF', 'NRC': 'NRC', 'ICRC': 'ICRC'\n",
    "    }\n",
    "    score_cols = ['aid_satisfaction', 'trust_in_aid_provider', 'communication_clarity', 'aid_fairness']\n",
    "    median_scores = df_clean[score_cols].median(skipna=True).apply(lambda x: int(round(x)))\n",
    "    \n",
    "    # Initialize audit columns\n",
    "    df_clean['processing_notes'] = ''\n",
    "    df_clean['is_valid'] = 1 # MySQL BOOLEAN/TINYINT True is 1\n",
    "    \n",
    "    # 1. Imputation and Type Casting (CRITICAL for NOT NULL)\n",
    "    for col in score_cols:\n",
    "        is_missing = df_clean[col].isna()\n",
    "        if is_missing.any():\n",
    "            imputed_value = median_scores[col]\n",
    "            df_clean.loc[is_missing, col] = imputed_value\n",
    "            df_clean.loc[is_missing, 'processing_notes'] += f\"Imputed missing {col}. \"\n",
    "            df_clean.loc[is_missing, 'is_valid'] = 0 # Mark as less valid due to imputation\n",
    "        # Force integer type for MySQL TINYINT\n",
    "        df_clean[col] = df_clean[col].astype(int) \n",
    "\n",
    "    # 2. Standardization\n",
    "    df_clean['gender'] = df_clean['gender'].astype(str).str.strip().replace(gender_map)\n",
    "    df_clean['aid_provider'] = df_clean['aid_provider'].astype(str).str.strip().replace(aid_provider_map)\n",
    "    \n",
    "    # 3. Final Cleanup (replace pandas NaN/None with standard Python None)\n",
    "    df_clean = df_clean.replace({np.nan: None})\n",
    "    \n",
    "    processed_cols = ['response_id', 'survey_date', 'location', 'aid_provider', 'displacement_status', \n",
    "                      'gender', 'age_group', 'aid_satisfaction', 'trust_in_aid_provider', \n",
    "                      'communication_clarity', 'aid_fairness', 'feedback_comment', 'is_valid', 'processing_notes']\n",
    "                      \n",
    "    return df_clean[processed_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954c69d-eb51-439f-89e8-5a1f8fb93d32",
   "metadata": {},
   "source": [
    "#### 3. ETL Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c86c2026-6c2d-4dcf-a31a-bd7396be2a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Extracting data from synthetic_gts_survey_data.csv...\n",
      "2. Applying data transformation and cleaning ...\n",
      "3. Connecting to MySQL and loading data...\n",
      "   -> Successfully loaded data into 'raw_responses'.\n",
      "   -> Successfully loaded cleaned data into 'gts_processed_data'.\n",
      "\n",
      "ETL Pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "def run_mysql_etl():\n",
    "    \"\"\"Executes the full ETL pipeline for both raw and processed tables.\"\"\"\n",
    "    \n",
    "    # E - Extract\n",
    "    print(f\"1. Extracting data from {CSV_FILE}...\")\n",
    "    try:\n",
    "        df_raw = pd.read_csv(CSV_FILE)\n",
    "        df_raw['survey_date'] = pd.to_datetime(df_raw['survey_date']).dt.date\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL: CSV file '{CSV_FILE}' not found.\")\n",
    "        return\n",
    "\n",
    "    # T - Transform (Phase 1.3)\n",
    "    print(\"2. Applying data transformation and cleaning ...\")\n",
    "    df_clean = transform_data(df_raw)\n",
    "    \n",
    "    # L - Load (Phase 1.2 & 1.3)\n",
    "    print(\"3. Connecting to MySQL and loading data...\")\n",
    "    try:\n",
    "        engine = create_engine(DATABASE_URL)\n",
    "        \n",
    "        # Load Raw Data (Phase 1.2) - Allows NaNs/NULLs for audit\n",
    "        df_raw_for_db = df_raw.replace({np.nan: None}) # Replace NaNs with None for SQL NULL\n",
    "        df_raw_for_db.to_sql('raw_responses', con=engine, if_exists='replace', index=False, chunksize=1000)\n",
    "        print(\"   -> Successfully loaded data into 'raw_responses'.\")\n",
    "        \n",
    "        # Load Cleaned Data (Phase 1.3) - Guaranteed no score NULLs\n",
    "        # Note: 'replace' drops and recreates the table, preserving schema integrity.\n",
    "        df_clean.to_sql('gts_processed_data', con=engine, if_exists='replace', index=False, chunksize=1000)\n",
    "        print(\"   -> Successfully loaded cleaned data into 'gts_processed_data'.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Database connection or loading error: {e}\")\n",
    "        print(\"Please check your .env credentials, MySQL server status, and table schemas.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nETL Pipeline complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_mysql_etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41e271-daf2-4001-804f-336bf2c73675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
